{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0140dc35-fcaa-48c5-90a6-41081c87f017",
   "metadata": {},
   "source": [
    "# Multimodal Emotion Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0f55de-84f8-495e-ab86-2dfe0617904d",
   "metadata": {},
   "source": [
    "A comprehensive multimodal emotional and psychological analysis system that integrates facial expression, body movement, voice tone and NLP to detect and interpret emotional and mental states from multiple input sources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a0cfde-37e5-498d-a253-0aee50a31566",
   "metadata": {},
   "source": [
    "#### import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75eabe36-dcd3-4de1-88fe-65d090398bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import dlib\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import timeit\n",
    "import datetime\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import sounddevice as sd\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f615e9a9-53f1-47bd-9458-9e6acdc2f834",
   "metadata": {},
   "source": [
    "## Import scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e413fc37-18e8-43cd-9fe8-fcab066905a4",
   "metadata": {},
   "source": [
    "### Video detection scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3184a305-24e4-41dc-a752-05989b17a639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model metadata saved to: models/expression/expression_model_RAF_metadata.pkl\n"
     ]
    }
   ],
   "source": [
    "# Face expression detection script related to the 'face_expression_recognition' repository\n",
    "from scripts.expression.face_script import * \n",
    "# Body movement detection script related to the 'body_movement_analysis' repository\n",
    "from scripts.body.body_script import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320024c4-35f1-422d-afea-68998d6f2af6",
   "metadata": {},
   "source": [
    "### Audio detection scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70c18dd6-b83d-4706-9780-287fe3cb1132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voice tone emotion detection script related to the 'voice_emotion_recognition' repository\n",
    "from scripts.voice.voice_script import *\n",
    "# NLP sentiment detection script related to the 'NLP_emotion_recognition' repository\n",
    "from scripts.nlp.nlp_script import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ac4633-0e08-46b9-a837-793d16cfbbc4",
   "metadata": {},
   "source": [
    "## Multimodal emotion detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7837c238-6efa-47a2-bd5b-ba9d3a7c2fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performs real-time emotion detection with user feedback using pre-trained model and video capture.\n",
    "def emotion_detection(model_metadata, img_size, emotion_frames):\n",
    "    \"\"\"\n",
    "    Initialize Face expression detection variables\n",
    "    \"\"\"\n",
    "    # Load Dlib's face detector\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    \n",
    "    # Initialize model and motions labels from pickle metadata\n",
    "    model = load_model(model_metadata[\"model_path\"])   # Load the pre-trained model \n",
    "    emotions = model_metadata[\"emotion_labels\"]  # Load the emotion classes\n",
    "    \n",
    "    # Initialize emotion count dictionary and variable for tracking total no. of emotions observed\n",
    "    # Used to return a summary of all observed emotions without relying on emotion stabilization \n",
    "    emotion_count = {label: 0 for label in emotions} # Emotion label dictionary \n",
    "    emotion_count_total = 0 \n",
    "    \n",
    "    # Initialize user key-press feedback (i.e. True or False emotion observed)\n",
    "    user_feedback = []             # List for emotion feedback\n",
    "    detected_emotion = None        # Track the last emotion that was logged\n",
    "\n",
    "    # Initialize variable for stabilizing emotions predicted by the model\n",
    "    # Used to ensure that predicted emotions are stable for a certain ammount of frames (i.e. 'emotion_frames' input argument)\n",
    "    # Helps to provide the users with enough time to identify and provide feedback for the observed emotion\n",
    "    emotion_queue = []   # FIFO queue for tracking observed emotions in the input frame count threshold, used as a buffer to stabilize emotion over time\n",
    "    emotion_prev = None  # Previous stabilized emotion from queue\n",
    "    emotion_curr = None  # Current stabilized emotion from queue\n",
    "    stable_frames = 0    # Counter for how many frames the emotion has been stable\n",
    "\n",
    "    \"\"\"\n",
    "    Initialize body movement detection variables\n",
    "    \"\"\"\n",
    "    mp_pose = mp.solutions.pose\n",
    "    pose = mp_pose.Pose()\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "    \n",
    "    # Initialize previous landmarks and analytics storage\n",
    "    landsmarks_prev = None\n",
    "    \n",
    "    # Initialize dictionary for movement data\n",
    "    movement_data = {}\n",
    "    \n",
    "    # Initialize time variables for identifying time of observations\n",
    "    start_time_body = time.time()  # Start time\n",
    "    max_time = 0  # Variable for time of maximum movement\n",
    "    max_movement = 0  # Variable for maximum movement score observed\n",
    "\n",
    "    \"\"\"\n",
    "    Initialize audio recording variables\n",
    "    \"\"\"\n",
    "    # Audio recording variables\n",
    "    sample_rate = 44100  # Sample rate\n",
    "    audio_recording = []  # List to store the recorded audio\n",
    "\n",
    "    # Terminal start-up prints\n",
    "    print(\"Starting audio recording...\") # Display recording in progress information\n",
    "    print(\"Video started: Press 'Q' to quit.\") # Display exit guide\n",
    "\n",
    "    # Start audio recording & webcam capture\n",
    "    with sd.InputStream(samplerate=sample_rate, channels=1, dtype='float32') as stream:\n",
    "        # Initialize video capture from webcam\n",
    "        cap = cv2.VideoCapture(0)\n",
    "\n",
    "        \"\"\"\n",
    "        Face expression detection from capture\n",
    "        \"\"\"\n",
    "        # Initial color for rectangle around detected face: Used to revert color change of user input key-press \n",
    "        # True: Green, False: Red, No input: Green\n",
    "        rect_color = (255, 0, 0) # Green color\n",
    "    \n",
    "        # Guide text for first 10 seconds\n",
    "        start_time = timeit.default_timer() # Start timer: Used to clear the text\n",
    "        text, font, scale, thickness = \"Press 'T' if the detected emotion is correct and 'F' if incorrect\", cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1\n",
    "        text_size = cv2.getTextSize(text, font, scale, thickness)[0]\n",
    "        text_pos = ((int(cap.get(cv2.CAP_PROP_FRAME_WIDTH )) - text_size[0]) // 2, int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT )) - 20) # Display in the middle at bottom\n",
    "        \n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "                        # Record audio chunks continuously\n",
    "            audio_chunk, overflowed = stream.read(int(sample_rate * 0.1))\n",
    "            if not overflowed:\n",
    "                audio_recording.append(audio_chunk)\n",
    "    \n",
    "            # Print user input instructions \n",
    "            if start_time is not None:\n",
    "                # Get elapsed time since starting video capture\n",
    "                elapsed_time = timeit.default_timer() - start_time\n",
    "                # Print 10 seconds\n",
    "                if elapsed_time  < 10:\n",
    "                    cv2.putText(frame, text, text_pos, font, scale, (180, 190, 180), thickness)\n",
    "                else:\n",
    "                    # Clear start_time variable afterwards \n",
    "                    start_time = None\n",
    "    \n",
    "            # Convert the frame to grayscale\n",
    "            gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "            # face detection:\n",
    "            # Detect faces using Dlib's face detector (performs better than the Haar Cascade Classifier previously used)\n",
    "            faces = detector(gray_frame)\n",
    "            # For each detected face (i.e. coordinates of detected face)\n",
    "            for face in faces:\n",
    "                # Capture face region and repare image for model classification\n",
    "                x, y, w, h = face.left(), face.top(), face.width(), face.height()\n",
    "                face_region = gray_frame[y:y+h, x:x+w]                   # Capture face region\n",
    "                face_resized = cv2.resize(face_region, img_size)         # Resize region to input image size\n",
    "                face_normalized = face_resized / 255.0                   # Normalize\n",
    "                face_reshaped = np.expand_dims(face_normalized, axis=-1) # Add channel dimension\n",
    "                face_input = np.expand_dims(face_reshaped, axis=0)       # Add batch dimension\n",
    "    \n",
    "                # Predict the emotion using the pre-trained model\n",
    "                prediction = model.predict(face_input, verbose=0)        # Pre-trained model loaded from the pickle file\n",
    "                prediction_max = np.argmax(prediction)                   # Index of emotion with the highest probability\n",
    "                prediction_label = emotions[prediction_max]              # Map emotion with the highest probability to the labels of the pickle file \n",
    "    \n",
    "                # Populate the emotion queue to obtained the max obs. emotion of the desired frames (i.e. the frame input argument)\n",
    "                # Stabilizes emotion prediction to enable user feedback\n",
    "                emotion_queue.append(prediction_label)   # Appends predicted emotions\n",
    "                if len(emotion_queue) > emotion_frames:   \n",
    "                    emotion_queue.pop(0)                 # Pop first emotion label if queue exceeds input frames\n",
    "                    \n",
    "                # Get the current emotion from the emotion prediction queue\n",
    "                emotion_curr = max(set(emotion_queue), key=emotion_queue.count)  # Max observed emotion label is the current (stable) emotion\n",
    "        \n",
    "                # Track changes in observed emotion\n",
    "                if emotion_curr == emotion_prev:\n",
    "                    # Increase stable frames value if the emotion labels remains the same\n",
    "                    stable_frames += 1          # Increment frames\n",
    "                else:\n",
    "                    # Reset stable frames value and previous label when a new emotion is observed. \n",
    "                    emotion_prev = emotion_curr # Update previous observed emotion label\n",
    "                    stable_frames = 1           # Reset frames\n",
    "      \n",
    "                # Detect stabilized emotions (i.e. when an emotion has been observed for longer than the emotion frame input threshold argument)\n",
    "                if stable_frames >= emotion_frames:\n",
    "                    if emotion_curr != detected_emotion:\n",
    "                        detected_emotion = emotion_curr  # Define or update the detected emotion\n",
    "    \n",
    "                # Display the predicted emotion top left corner\n",
    "                if detected_emotion:\n",
    "                    cv2.putText(frame, detected_emotion, (265, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (100, 255, 100), 2)\n",
    "                    \n",
    "                # Display rectangle around the face region with current color\n",
    "                cv2.rectangle(frame, (x, y), (x+w, y+h), rect_color, 2)\n",
    "                \n",
    "                # Reset rectangle color to blue\n",
    "                if stable_frames > 5:\n",
    "                    rect_color = (255, 0, 0)\n",
    "    \n",
    "                # Update emotion label and total counters\n",
    "                emotion_count[prediction_label] += 1  # Emotion label individual counts\n",
    "                emotion_count_total += 1              # Emotion label individual counts\n",
    "    \n",
    "            # Display detection text\n",
    "            cv2.putText(frame, \"Detect emotion:\", (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (100, 255, 100), 2)\n",
    "\n",
    "            \n",
    "            \"\"\"\n",
    "            Body movement detection from capture\n",
    "            \"\"\"\n",
    "            # Body movement detection:\n",
    "            # Process image to retrieve pose landmarks\n",
    "            image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            image_pose = pose.process(image_rgb)\n",
    "    \n",
    "            # Check if landmarks are detected\n",
    "            if image_pose.pose_landmarks:\n",
    "                # Draw detected landmarks \n",
    "                mp_drawing.draw_landmarks(\n",
    "                    frame, image_pose.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "        \n",
    "                # Extract current landmark\n",
    "                landmarks_curr = image_pose.pose_landmarks.landmark\n",
    "        \n",
    "                # Detect and analyse body part movement obtained with the landmarks\n",
    "                if landsmarks_prev is not None:\n",
    "                    # Call movement calculator function to get differrence between current and prevous landmark\n",
    "                    movement = calculate_movement(landmarks_curr, landsmarks_prev)\n",
    "                    movement_total = sum(movement.values())  # Total movement score\n",
    "        \n",
    "                    # Call movement categorizer function to obtain total movement classification score (low, med, high) \n",
    "                    movement_class, _ = categorize_movement(movement_total)\n",
    "        \n",
    "                    # Summarize body part movement\n",
    "                    movement_parts = {\n",
    "                        'Head': movement['Left Eye'] + movement['Left Eye'],  # Head movement score\n",
    "                        'Shoulders': movement['Left Shoulder'] + movement['Right Shoulder'],  # Shoulders movement score\n",
    "                        'Hands': movement['Left Wrist'] + movement['Right Wrist'],  # Hands movement score\n",
    "                        'Elbows': movement['Left Elbow'] + movement['Right Elbow'],  # Elbows movement score\n",
    "                        'Hips': movement['Left Hip'] + movement['Right Hip'],  # Hips movement score\n",
    "                        'Knees': movement['Left Knee'] + movement['Right Knee'],  # Knees movement score\n",
    "                        'Ankles': movement['Left Ankle'] + movement['Right Ankle']  # Ankles movement score\n",
    "                    }\n",
    "                    \n",
    "                    # Identify most moved body part\n",
    "                    movement_max = max(movement_parts, key=movement_parts.get)\n",
    "        \n",
    "                    # Check if this is the maximum movement so far\n",
    "                    current_time = time.time() - start_time_body\n",
    "                \n",
    "                    # Check if current overall movement score is higher than the previous max observer\n",
    "                    if movement_total > max_movement:\n",
    "                        max_movement = movement_total  # Update if new max is observed\n",
    "                        max_time = current_time  # Update obervation time\n",
    "        \n",
    "                    # Display overall movement classification score on video capture\n",
    "                    cv2.putText(\n",
    "                        frame, f'Movement: {movement_class}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA\n",
    "                    )\n",
    "                    \n",
    "                    # Display time of maximum movement observed\n",
    "                    cv2.putText(\n",
    "                        frame, f'Max Movement at: {max_time:.2f} sec', (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA\n",
    "                    )\n",
    "        \n",
    "                    # Store the movement data to the dictionary \n",
    "                    movement_data['total_movement'] = movement_total  # Overall movement score (i.e. sum of movement values)\n",
    "                    movement_data['total_movement_class'] = movement_class  # Overall movement classification\n",
    "                    movement_data['most_moved_part'] = movement_max  # Most moved body part class\n",
    "                    movement_data['most_moved_score'] = movement_parts[movement_max]  # Most moved body part score\n",
    "                    movement_data['individual_movements'] = movement  # Individual body part scores\n",
    "    \n",
    "                # Update previous landmarks\n",
    "                landsmarks_prev = landmarks_curr\n",
    "    \n",
    "            # Display image with face expression classification and body movement annotation \n",
    "            cv2.imshow('Facial Expression Detection', frame)\n",
    "    \n",
    "            # Handle key presses\n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            # Press 'T' to mark the the current stable emotion as false\n",
    "            if key == ord('q'):  \n",
    "                break\n",
    "\n",
    "        # Release the video capture object and close all OpenCV windows\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        \n",
    "        \"\"\"\n",
    "        Audio recording post-processing\n",
    "        \"\"\"\n",
    "        # Save the recorded audio to a wav file\n",
    "        if audio_recording:\n",
    "            audio_data = np.concatenate(audio_recording, axis=0)\n",
    "            output_filename = \"data/audio/recording.wav\"\n",
    "            sf.write(output_filename, audio_data, sample_rate)\n",
    "            print(f\"Audio saved as {output_filename}\")\n",
    "        else:\n",
    "            print(\"No audio recorded.\")\n",
    "\n",
    "        \n",
    "        \"\"\"\n",
    "        Face expression detection post-processing\n",
    "        \"\"\"\n",
    "        # Calculate percentages and determine the overall state\n",
    "        emotion_summary = {} # Dictionary for facial expressions predictions \n",
    "        overall_state = None\n",
    "\n",
    "        if emotion_count_total > 0:\n",
    "            overall_state = max(emotion_count, key=emotion_count.get)  # Get most frequently detected emotion\n",
    "            # Save percentage to dictionary for each emotional state\n",
    "            for emotion, count in emotion_count.items():\n",
    "                percentage = (count / emotion_count_total) * 100\n",
    "                emotion_summary[emotion] = f\"{percentage:.2f}%\"\n",
    "        else:\n",
    "            emotion_summary = {\"message\": \"No emotions were detected.\"}\n",
    "\n",
    "        \"\"\"\n",
    "        Body movement detection post-processing\n",
    "        \"\"\"\n",
    "        # Body movement analysis dictionary initialized with summary vaules\n",
    "        movement_analysis = {\n",
    "            \"overall_movement_score\": f\"{movement_data.get('total_movement', 0):.2f}\",\n",
    "            \"overall_movement_classification\": movement_data.get('total_movement_class', 'N/A'),\n",
    "            \"max_time\": f\"{max_time:.2f} sec\",\n",
    "            \"most_moved_part\": movement_data.get('most_moved_part', 'N/A'),\n",
    "            \"most_moved_part_score\": f\"{movement_data.get('most_moved_score', 0):.2f}\"\n",
    "        }\n",
    "\n",
    "        # Add individual movements scores to the body movement analysis dictionary\n",
    "        individual_movements = {}\n",
    "        for part, movement in movement_data.get('individual_movements', {}).items():\n",
    "            individual_movements[part] = f\"{movement:.2f}\"\n",
    "        movement_analysis[\"individual_movements\"] = individual_movements\n",
    "\n",
    "        \n",
    "        \"\"\"\n",
    "        Voice tone detection: Detects voice tone emotions based on the audio features of the saved audio recording.\n",
    "        \"\"\"\n",
    "        tone_prediction = voice_prediction(\"data/audio/recording.wav\")\n",
    "\n",
    "        \"\"\"\n",
    "        NLP sentiment analysis: Detects speech-to-text sentiment based on the saved audio recording.\n",
    "        \"\"\"\n",
    "        nlp_prediction = classify_audio_emotion(\n",
    "            audio_file_path=\"data/audio/recording.wav\",\n",
    "            whisper_model=whisper_model,\n",
    "            emotion_model=emotion_model,\n",
    "            tokenizer=tokenizer,\n",
    "            stop_words=stop_words_nltk,\n",
    "            max_len=178,\n",
    "            threshold=0.5\n",
    "        )\n",
    "        \n",
    "        # Return multimodal detections\n",
    "        return emotion_summary, movement_analysis, tone_prediction, nlp_prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34fcb70f-ac15-43ef-a234-066448e55d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting audio recording...\n",
      "Video started: Press 'Q' to quit.\n",
      "Audio saved as data/audio/recording.wav\n"
     ]
    }
   ],
   "source": [
    "emotion_summary, movement_analysis, tone_prediction, nlp_prediction = emotion_detection(model_metadata, (100, 100), 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819cfa00-edb6-4f52-9d82-59f8558e5a80",
   "metadata": {},
   "source": [
    "## Evalution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64699ab4-a455-4f23-aece-80e64173612d",
   "metadata": {},
   "source": [
    "### Evaluting 'happiness'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0c77691-8619-4dd4-a938-c80802d62d9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Surprise': '0.00%',\n",
       " 'Fear': '0.00%',\n",
       " 'Disgust': '0.00%',\n",
       " 'Happiness': '100.00%',\n",
       " 'Sadness': '0.00%',\n",
       " 'Anger': '0.00%',\n",
       " 'Neutral': '0.00%'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Facial expression\n",
    "emotion_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "007169ee-892b-4f2c-9d64-588ec0a3a224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'overall_movement_score': '0.05',\n",
       " 'overall_movement_classification': 'Low',\n",
       " 'max_time': '4.76 sec',\n",
       " 'most_moved_part': 'Elbows',\n",
       " 'most_moved_part_score': '0.03',\n",
       " 'individual_movements': {'Left Eye': '0.00',\n",
       "  'Right Eye': '0.00',\n",
       "  'Left Shoulder': '0.00',\n",
       "  'Right Shoulder': '0.00',\n",
       "  'Left Elbow': '0.01',\n",
       "  'Right Elbow': '0.02',\n",
       "  'Left Wrist': '0.00',\n",
       "  'Right Wrist': '0.00',\n",
       "  'Left Hip': '0.00',\n",
       "  'Right Hip': '0.00',\n",
       "  'Left Knee': '0.00',\n",
       "  'Right Knee': '0.00',\n",
       "  'Left Ankle': '0.00',\n",
       "  'Right Ankle': '0.00'}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Movement \n",
    "movement_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f039c3c6-464f-41e5-9f60-3c1a855eac82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'angry': '0.00',\n",
       " 'disgust': '0.75',\n",
       " 'fear': '1.79',\n",
       " 'happy': '0.04',\n",
       " 'neutral': '89.75',\n",
       " 'sad': '7.19',\n",
       " 'surprise': '0.48',\n",
       " 'predicted_emotion': 'neutral'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Voice tone \n",
    "tone_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "138b7809-2177-4026-aeb0-41152f377c24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'transcribed_text': 'I am very happy about the good weather today.',\n",
       " 'predicted_probabilities': {'disgust': '0.00',\n",
       "  'fear': '0.00',\n",
       "  'anger': '0.00',\n",
       "  'joy': '100.00',\n",
       "  'sadness': '0.00',\n",
       "  'surprise': '0.00'},\n",
       " 'predicted_emotion': 'joy'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLP sentiment\n",
    "nlp_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4943aa85-4b49-454f-b82b-e8415389dd2c",
   "metadata": {},
   "source": [
    "### Evaluting 'sadness'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "090f79f1-fa79-44dd-82f3-4db325aef9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting audio recording...\n",
      "Video started: Press 'Q' to quit.\n",
      "Audio saved as data/audio/recording.wav\n"
     ]
    }
   ],
   "source": [
    "emotion_summary, movement_analysis, tone_prediction, nlp_prediction = emotion_detection(model_metadata, (100, 100), 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f05aade-6328-4f42-a958-636a25bf6e06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Surprise': '0.00%',\n",
       " 'Fear': '0.00%',\n",
       " 'Disgust': '0.00%',\n",
       " 'Happiness': '0.00%',\n",
       " 'Sadness': '82.27%',\n",
       " 'Anger': '0.00%',\n",
       " 'Neutral': '17.73%'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Facial expression\n",
    "emotion_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af6e253b-0b68-4826-96f4-cd92c52cfd85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'overall_movement_score': '0.08',\n",
       " 'overall_movement_classification': 'Low',\n",
       " 'max_time': '17.34 sec',\n",
       " 'most_moved_part': 'Ankles',\n",
       " 'most_moved_part_score': '0.03',\n",
       " 'individual_movements': {'Left Eye': '0.00',\n",
       "  'Right Eye': '0.00',\n",
       "  'Left Shoulder': '0.00',\n",
       "  'Right Shoulder': '0.00',\n",
       "  'Left Elbow': '0.00',\n",
       "  'Right Elbow': '0.01',\n",
       "  'Left Wrist': '0.00',\n",
       "  'Right Wrist': '0.00',\n",
       "  'Left Hip': '0.00',\n",
       "  'Right Hip': '0.01',\n",
       "  'Left Knee': '0.01',\n",
       "  'Right Knee': '0.01',\n",
       "  'Left Ankle': '0.01',\n",
       "  'Right Ankle': '0.01'}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Movement \n",
    "movement_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b544ce3-7c11-4fe4-a253-6d7938c26bfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'angry': '0.00',\n",
       " 'disgust': '0.00',\n",
       " 'fear': '0.05',\n",
       " 'happy': '0.02',\n",
       " 'neutral': '97.64',\n",
       " 'sad': '2.04',\n",
       " 'surprise': '0.26',\n",
       " 'predicted_emotion': 'neutral'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Voice tone \n",
    "tone_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d79a3272-3b54-4fc0-a13b-0b85f1514665",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'transcribed_text': \"I'm really sad about the battles that came today.\",\n",
       " 'predicted_probabilities': {'disgust': '0.00',\n",
       "  'fear': '0.00',\n",
       "  'anger': '0.00',\n",
       "  'joy': '0.00',\n",
       "  'sadness': '100.00',\n",
       "  'surprise': '0.00'},\n",
       " 'predicted_emotion': 'sadness'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLP sentiment\n",
    "nlp_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8749318b-2b99-4cd8-a23c-11bf980d0bd2",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87235ff8-b9f0-41ec-8d96-62e17cd291b1",
   "metadata": {},
   "source": [
    "The Multimodal Emotion detection extracts information from video and audio recordings. It effectively captures a variety of information about the emotional state of the patient. These observations can be used to provide valuable insight when conducting a psychiatric assessment or to help classify critical sections of an interview."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow GPU Kernel 2",
   "language": "python",
   "name": "tensor-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
